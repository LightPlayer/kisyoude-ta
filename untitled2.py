# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16tbuHWZRw8_dYUvqYJW_XOY2QJSTT8kU
"""

import numpy as np

import matplotlib.pyplot as plt

def sigmoid(x):
    return 1/(1+np.exp(-x))

x=np.arange(-10,10,0.1)
plt.plot(x,sigmoid(x))

import torch
X= torch.Tensor([[0,0],[0,1],[1,0],[1,1]])
t= torch.Tensor([[0],[1],[1],[1]])
X

t

from torch import nn

model = nn.Sequential(
    nn.Linear(2,1),
    nn.Sigmoid()
)
y=model(X)

y

loss_fn=nn.MSELoss()
loss_fn(y,t)

from torch import optim
optimizer = optim.SGD(model.parameters(), lr=0.1)

model.train()
for epoch in range(100000):
    optimizer.zero_grad()
    y=model(X)
    loss = loss_fn(y,t)

    loss.backward()
    optimizer.step()

model(X)

t

loss_fn(model(X),t)

X=torch.Tensor([[0,0],[0,1],[1,0],[1,1]])
t=torch.Tensor([[0],[1],[1],[0]])

model = nn.Sequential(
    nn.Linear(2,2),
    nn.Sigmoid(),
    nn.Linear(2,1),
    nn.Sigmoid()
)

optimizer=optim.SGD(model.parameters(),lr=1)

model.train()
for epoch in range(50000):
  optimizer.zero_grad()
  y=model(X)
  loss=loss_fn(y,t)


  loss.backward()
  optimizer.step()

print(model(X))
print(t)
print(y)
print(loss_fn(y,t))

#ここから本番用

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd 
data= pd.read_csv('drive/My Drive/data/kisyoudata.csv')

data

# -*- coding: utf-8 -*-
from google.colab import  files
files.upload()
import os
import datetime
import csv
import urllib.request
from bs4 import BeautifulSoup

def str2float(weather_data):
    try:
        return float(weather_data)
    except:
        return 0

def scraping(url, date):

    # 気象データのページを取得
    html = urllib.request.urlopen(url).read()
    soup = BeautifulSoup(html)
    trs = soup.find("table", { "class" : "data2_s" })

    data_list = []
    data_list_per_hour = []

    # table の中身を取得
    for tr in trs.findAll('tr')[2:]:
        tds = tr.findAll('td')

        if tds[1].string == None:
            break;

        data_list.append(date)
        data_list.append(tds[0].string)
        data_list.append(str2float(tds[1].string))
        data_list.append(str2float(tds[2].string))
        data_list.append(str2float(tds[3].string))
        data_list.append(str2float(tds[4].string))
        data_list.append(str2float(tds[5].string))
        data_list.append(str2float(tds[6].string))
        data_list.append(str2float(tds[7].string))
        data_list.append(str2float(tds[8].string))
        data_list.append(str2float(tds[9].string))
        data_list.append(str2float(tds[10].string))
        data_list.append(str2float(tds[11].string))
        data_list.append(str2float(tds[12].string))
        data_list.append(str2float(tds[13].string))

        data_list_per_hour.append(data_list)

        data_list = []

    return data_list_per_hour

def create_csv():
    # CSV 出力先ディレクトリ
    output_dir = 'drive/My Drive/data/'

    # 出力ファイル名
    output_file = "kisyoudata.csv"

    # データ取得開始・終了日
    start_date = datetime.date(2000, 1, 1)
    end_date   = datetime.date(2020, 12, 31)

    # CSV の列
    fields = ["年月日", "時間", "気圧（現地）", "気圧（海面）",
              "降水量", "気温", "湿度",
              "風速", "風向", "日照時間"] # 天気、雲量、視程は今回は対象外とする

    with open("kisyoudata.csv", 'w') as f:
        writer = csv.writer(f, lineterminator='\n')
        writer.writerow(fields)

        date = start_date
        while date != end_date + datetime.timedelta(1):

            # 対象url（今回は東京）
            url = "http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?" \
                  "prec_no=44&block_no=47662&year=%d&month=%d&day=%d&view="%(date.year, date.month, date.day)

            data_per_day = scraping(url, date)

            for dpd in data_per_day:
                writer.writerow(dpd)

            date += datetime.timedelta(1)

if __name__ == '__main__':
    create_csv()